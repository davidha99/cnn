{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto final (CNN)\n",
    "\n",
    "Omar David Hernández Aguirre  | A01383543  \n",
    "Bernardo García Zermeño       | A00570682  \n",
    "06 de junio de 2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from  torch.utils import data\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "\n",
    "# Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# For training purposes took first 5 classes. Taking all 43 takes a lot of time. \n",
    "NUM_CLASSES = 43\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "Get a list of all the paths of the training images, classified by different number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = {}\n",
    "for i in range(NUM_CLASSES):\n",
    "    path = './archive/Train/' + str(i)\n",
    "    images = os.listdir(path)\n",
    "    train_ids[i] = []\n",
    "    # print(images)\n",
    "    # print(train_ids)\n",
    "\n",
    "    for img in images:\n",
    "            img_path = path + '/' + img\n",
    "            train_ids[i].append(img_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use DataLoader, we need to get info from a torch Dataset. Pytorch allows us to create our own customize dataset.\n",
    "A customized Dataset is taken for practical purposes from TUSSALO: https://www.kaggle.com/code/tussalo/gtsrb-99-05-test-accuracy-with-efficient-cnns\n",
    "\n",
    "TUSSALO includes other methods for extra functionalities that we're not going to use, but this customized dataset allows us to include our imgs in a Pytorch Dataset and use it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB_Dataset(Dataset):\n",
    "    \n",
    "    def load_image(self,key_path):\n",
    "        \"\"\"\n",
    "        key_path --> Tuple conisting of a key, which is the label of the traffic-sign and the path to the image.\n",
    "        Loads the image and converts it into a usable torch.tensor. \n",
    "        Returns label and tensor.\n",
    "        \"\"\"\n",
    "        image = torch.tensor(cv2.imread(key_path[1]),dtype = torch.float)/255\n",
    "        image = image.transpose(0,2)\n",
    "        image = image.transpose(1,2)\n",
    "        # By default RGB channels are in BGR order, so needs to be flipped\n",
    "        image = image.flip(0)\n",
    "        return key_path[0],image\n",
    "    \n",
    "    def augment_image(self, key_image):\n",
    "        \"\"\"\n",
    "        key_image --> Tuple of Label and the image-tensor.\n",
    "        \n",
    "        Only if a transformation is defined during initialization of the GTSRB_Dataset, an augmentation will be performed.\n",
    "        Only if a dict with symmetric classes is given, horizontal flipping will be performed.\n",
    "        Only if a dict with mirrors is given, horizontal flip and label reassignment will be performed.\n",
    "        \n",
    "        The augmentation is allways based on the raw image tensor, which is not yet resized! Resizing will be performed on \n",
    "        the augmented data in this function. \n",
    "        \n",
    "        Returns a Label (which might have changed due to mirrors) and the augmented image of uniform size, defined during\n",
    "        initialization of the GTSRB_Dataset (default is 32x32).\n",
    "        \n",
    "        Augmentation is not determistic so a different result will be achieved each time. (probably)\n",
    "        Creating a deterministic augmentation would be great for reproducibility, but it needs to be different in each iteration\n",
    "        of the augmentation.\n",
    "        \n",
    "        \"\"\"\n",
    "        label,image = key_image\n",
    "        if self.transformation:\n",
    "            if self.symmetric_classes[str(label).zfill(2)]:\n",
    "                # self.hflipper is defined during init with probability 0.5\n",
    "                image = self.hflipper(image)\n",
    "            elif self.mirrored_classes[str(label).zfill(2)]:\n",
    "                # If a sign of class A can be flipped to represent a sign of class B, the probability of this A-Sign being flipped is \n",
    "                # n(B)/(n(A) + n(B)) --> with n(X) meaning the absolute number of occurences of the class X in the Dataset\n",
    "                # Thereby the number of unflipped signs in class A + flipped signs from class B and relabeld as A is roughly n(A).\n",
    "                if np.random.rand() > (self.priors[str(label).zfill(2)]/(self.priors[str(label).zfill(2)] + self.priors[self.mirrored_classes[str(label).zfill(2)]])):\n",
    "                    image = T.functional.hflip(image)\n",
    "                    label = int(self.mirrored_classes[str(label).zfill(2)])\n",
    "            width = image.shape[1]\n",
    "            height = image.shape[2]\n",
    "            # transformation is defined outside of the dataset to allow simple adjustments, see below\n",
    "            image = self.transformation(image)\n",
    "            # RandomCrop to 95% to induce translational movement and compensated the zoom-out induced by the random perspective\n",
    "            image = T.RandomCrop((int(width*.95), int(height*.95)))(image)\n",
    "        return(label, resize(image, self.im_size))\n",
    "\n",
    "    \n",
    "    def __init__(self, data_ids,  mirrored_classes, symmetric_classes, augmentation_size, transformation, im_size = [32, 32], device = \"cpu\"):\n",
    "        \"\"\"\n",
    "        data_ids --> dict of labels and corresponding image paths used for the dataset.\n",
    "        mirrored_classes --> dict of labels and one corresponding label or None, determines for each class which other one it represents after flipping horizontally\n",
    "        symmetric_classes --> dict of labels and boolean value if they can be flipped horizontally\n",
    "        augmentation_size --> int that determines number of augmented versions of each image to be added to the raw data\n",
    "        transformation --> torchvision.transformation which contains a list of imagetransformations to be used during augmentation\n",
    "        im_size --> Tuple with number of Pixels in x and y direction each image will be resized to, default : (32,32)\n",
    "        device --> torch.device, cuda will be used by default \n",
    "        \n",
    "        First the number of occurences of eacht class (priors) is counted. --> will be used with mirrors to calculate probability\n",
    "        A pool of 8 threads is used to speed up the image loading process.\n",
    "        A list of all tuples (label, image_tensor) is created: raw_data\n",
    "        Afterwards there will be (augmentation_size)-iterations over the raw_data, augmenting eacht image individually (see function augment image)\n",
    "        raw_data and augmented_data are stacked together to data tensor with all the images and label tensor containing the label for each index.\n",
    "        \n",
    "        These two tensors are used as the mapstyle-dataset.        \n",
    "        The whole dataset is also moved to the specified device, e.g. gpu, in order to avoid memory bottlenecks during training.\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.priors = {str(key).zfill(2) : len(paths) for key,paths in data_ids.items()}\n",
    "        self.image_paths = [(key,path) for key,paths in data_ids.items() for path in paths]\n",
    "        self.mirrored_classes = mirrored_classes\n",
    "        self.symmetric_classes = symmetric_classes\n",
    "        self.im_size = im_size\n",
    "        self.transformation = transformation\n",
    "        pool = ThreadPool(8)\n",
    "        self.raw_data = pool.map(self.load_image, self.image_paths)\n",
    "\n",
    "        self.augmented_data = []\n",
    "\n",
    "        if augmentation_size > 0:\n",
    "            self.hflipper =  T.RandomHorizontalFlip(p=0.5)\n",
    "            for i in range(augmentation_size):\n",
    "                # I could not find a way to create a deterministic augmentation while using the pool of threads. It worked without the multithreading but just took to long.\n",
    "                self.augmented_data += pool.map(self.augment_image, self.raw_data)\n",
    "\n",
    "        \n",
    "        self.data = torch.stack([resize(image, im_size) for _,image in self.raw_data] + [image for _,image in self.augmented_data]).to(torch.device(device))\n",
    "        self.labels = torch.tensor([int(label) for label,_ in self.raw_data] + [int(label) for label,_ in self.augmented_data]).to(torch.uint8).to(torch.device(device))\n",
    "        del self.raw_data, self.augmented_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide out training and test data, and create the DataLoaders for each to work with our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7842\n"
     ]
    }
   ],
   "source": [
    "training_dataset = GTSRB_Dataset(train_ids,None,None,0,None)\n",
    "train_data_size = 0.8 * len(training_dataset)\n",
    "test_data_size =  len(training_dataset) - int(train_data_size)\n",
    "# print(test_data_size)\n",
    "train_data, test_data = data.random_split(training_dataset, [int(train_data_size), int(test_data_size)])\n",
    "trainloader = DataLoader(train_data, batch_size=64)\n",
    "testloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "2 feature learning sections\n",
    "1 classification section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(in_channels= 3, out_channels=16, kernel_size=3, stride=1,bias = True),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels= 16, out_channels=32, kernel_size=3, stride=1,bias = True),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size = 2),\n",
    "          nn.BatchNorm2d(32),\n",
    "\n",
    "          nn.Conv2d(in_channels= 32, out_channels=64, kernel_size=3, stride=1,bias = True),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,bias = True),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size = 2),\n",
    "          nn.BatchNorm2d(128),\n",
    "\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(in_features=128*25, out_features=512, bias = True),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(512),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Linear(in_features = 512, out_features = n_classes, bias = True)        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, lossfn, optimizer, print_outputs = True, epochs = 5):\n",
    "    \"\"\"\n",
    "    Description:   \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracy = []    \n",
    "        \n",
    "    # Counters to keep accuracy tracking\n",
    "    correct_train = 0\n",
    "    loss_train = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # print(X.size())\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred=model(X)\n",
    "        loss = lossfn(pred, y)\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keeping track of the num of correct predictions\n",
    "        _,pred_idxs = torch.topk(pred, 1)\n",
    "        correct_train += torch.eq(y, pred_idxs.squeeze()).sum().item()\n",
    "\n",
    "    acc_train = correct_train/len(trainloader)\n",
    "    loss_train = loss_train/len(trainloader)\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracy.append(acc_train)\n",
    "\n",
    "\n",
    "    return acc_train, loss_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(size)\n",
    "    num_batches = len(dataloader)   # Obtenemos el num batches para el analisis porcentual del error\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0       # Variables para el analisis del error\n",
    "    with torch.no_grad():           # A pesar de poner el model.eval(), este es un step mas para que el modelo no aprenda de lo que va a hacer\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Keeping track of the num of correct predictions\n",
    "            _,pred_idxs = torch.topk(pred, 1)\n",
    "            correct += torch.eq(y, pred_idxs.squeeze()).sum().item()\n",
    "            # print(correct)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 0  Accuracy: 28.036659877800407 Avg Loss: 2.320025452528369 \n",
      "Training - Epoch: 1  Accuracy: 48.08757637474542 Avg Loss: 1.1166371111957216 \n",
      "Training - Epoch: 2  Accuracy: 55.30142566191446 Avg Loss: 0.6930267330099754 \n",
      "Training - Epoch: 3  Accuracy: 58.70264765784114 Avg Loss: 0.47820855109609073 \n",
      "Training - Epoch: 4  Accuracy: 60.27902240325866 Avg Loss: 0.36155581313456636 \n",
      "Training - Epoch: 5  Accuracy: 61.329938900203665 Avg Loss: 0.2833914452985445 \n",
      "Training - Epoch: 6  Accuracy: 61.93482688391039 Avg Loss: 0.2302160084763515 \n",
      "Training - Epoch: 7  Accuracy: 62.372708757637476 Avg Loss: 0.19296907251909406 \n",
      "Training - Epoch: 8  Accuracy: 62.70672097759674 Avg Loss: 0.16525382219288354 \n",
      "Training - Epoch: 9  Accuracy: 62.91649694501018 Avg Loss: 0.14263191367755595 \n",
      "Training - Epoch: 10  Accuracy: 63.057026476578415 Avg Loss: 0.12556423173265155 \n",
      "Training - Epoch: 11  Accuracy: 63.193482688391036 Avg Loss: 0.11166406928916327 \n",
      "Training - Epoch: 12  Accuracy: 63.23014256619145 Avg Loss: 0.10111481135676932 \n",
      "Training - Epoch: 13  Accuracy: 63.31568228105906 Avg Loss: 0.09191769442678469 \n",
      "Training - Epoch: 14  Accuracy: 63.435845213849284 Avg Loss: 0.08311625376586516 \n"
     ]
    }
   ],
   "source": [
    "model = CNN(NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# # Create a loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# # Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),  # parameters of target model to optimize\n",
    "                            lr=0.001)\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    acc_train, loss_train = train(model, trainloader, loss_fn, optimizer)\n",
    "    print(f\"Training - Epoch: {t}  Accuracy: {acc_train} Avg Loss: {loss_train} \")\n",
    "\n",
    "\n",
    "test(testloader, model, loss_fn)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main steps:\n",
    "1. X Data preparation: make images into tensors with DataLoader \n",
    "2. X Make train set and test set\n",
    "3. X Make the model\n",
    "4. X Train the model\n",
    "5. X Test the model\n",
    "6. Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 1:'Speed limit (20km/h)',\n",
    "            2:'Speed limit (30km/h)',\n",
    "            3:'Speed limit (50km/h)',\n",
    "            4:'Speed limit (60km/h)',\n",
    "            5:'Speed limit (70km/h)',\n",
    "            6:'Speed limit (80km/h)',\n",
    "            7:'End of speed limit (80km/h)',\n",
    "            8:'Speed limit (100km/h)',\n",
    "            9:'Speed limit (120km/h)',\n",
    "            10:'No passing',\n",
    "            11:'No passing veh over 3.5 tons',\n",
    "            12:'Right-of-way at intersection',\n",
    "            13:'Priority road',\n",
    "            14:'Yield',\n",
    "            15:'Stop',\n",
    "            16:'No vehicles',\n",
    "            17:'Veh > 3.5 tons prohibited',\n",
    "            18:'No entry',\n",
    "            19:'General caution',\n",
    "            20:'Dangerous curve left',\n",
    "            21:'Dangerous curve right',\n",
    "            22:'Double curve',\n",
    "            23:'Bumpy road',\n",
    "            24:'Slippery road',\n",
    "            25:'Road narrows on the right',\n",
    "            26:'Road work',\n",
    "            27:'Traffic signals',\n",
    "            28:'Pedestrians',\n",
    "            29:'Children crossing',\n",
    "            30:'Bicycles crossing',\n",
    "            31:'Beware of ice/snow',\n",
    "            32:'Wild animals crossing',\n",
    "            33:'End speed + passing limits',\n",
    "            34:'Turn right ahead',\n",
    "            35:'Turn left ahead',\n",
    "            36:'Ahead only',\n",
    "            37:'Go straight or right',\n",
    "            38:'Go straight or left',\n",
    "            39:'Keep right',\n",
    "            40:'Keep left',\n",
    "            41:'Roundabout mandatory',\n",
    "            42:'End of no passing',\n",
    "            43:'End no passing veh > 3.5 tons' }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
